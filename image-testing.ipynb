{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "from mediapipe import solutions\n",
    "from mediapipe.framework.formats import landmark_pb2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MARGIN = 10  # pixels\n",
    "FONT_SIZE = 1\n",
    "FONT_THICKNESS = 1\n",
    "HANDEDNESS_TEXT_COLOR = (88, 205, 54)  # vibrant green\n",
    "\n",
    "\n",
    "def draw_landmarks_on_image(rgb_image, detection_result):\n",
    "    hand_landmarks_list = detection_result.hand_landmarks\n",
    "    handedness_list = detection_result.handedness\n",
    "    annotated_image = np.copy(rgb_image)\n",
    "\n",
    "    # Loop through the detected hands to visualize.\n",
    "    for idx in range(len(hand_landmarks_list)):\n",
    "        hand_landmarks = hand_landmarks_list[idx]\n",
    "        handedness = handedness_list[idx]\n",
    "\n",
    "        # Draw the hand landmarks.\n",
    "        hand_landmarks_proto = landmark_pb2.NormalizedLandmarkList()\n",
    "        hand_landmarks_proto.landmark.extend([\n",
    "            landmark_pb2.NormalizedLandmark(x=landmark.x, y=landmark.y, z=landmark.z) for landmark in hand_landmarks\n",
    "        ])\n",
    "        solutions.drawing_utils.draw_landmarks(\n",
    "            annotated_image,\n",
    "            hand_landmarks_proto,\n",
    "            solutions.hands.HAND_CONNECTIONS,\n",
    "            solutions.drawing_styles.get_default_hand_landmarks_style(),\n",
    "            solutions.drawing_styles.get_default_hand_connections_style())\n",
    "\n",
    "        # Get the top left corner of the detected hand's bounding box.\n",
    "        height, width, _ = annotated_image.shape\n",
    "        x_coordinates = [landmark.x for landmark in hand_landmarks]\n",
    "        y_coordinates = [landmark.y for landmark in hand_landmarks]\n",
    "        text_x = int(min(x_coordinates) * width)\n",
    "        text_y = int(min(y_coordinates) * height) - MARGIN\n",
    "\n",
    "        # Draw handedness (left or right hand) on the image.\n",
    "        # cv2.putText(annotated_image, f\"{handedness[0].category_name}\",\n",
    "        #             (text_x, text_y), cv2.FONT_HERSHEY_DUPLEX,\n",
    "        #             FONT_SIZE, HANDEDNESS_TEXT_COLOR, FONT_THICKNESS, cv2.LINE_AA)\n",
    "\n",
    "    return annotated_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def landmark_array(landmarks):\n",
    "    # Initialize empty array to store landmarks\n",
    "    arr = np.empty((21, 2))\n",
    "    \n",
    "    # Save landmarks to array\n",
    "    for index, landmark in enumerate(landmarks):\n",
    "        arr[index][0] = landmark.x\n",
    "        arr[index][1] = landmark.y\n",
    "        \n",
    "    return arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define labels\n",
    "labels = ['A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N',\n",
    "          'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', 'space', 'del']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load model\n",
    "model_left = tf.keras.models.load_model('model-left.h5')\n",
    "model_right = tf.keras.models.load_model('model-right.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "BaseOptions = mp.tasks.BaseOptions\n",
    "HandLandmarker = mp.tasks.vision.HandLandmarker\n",
    "HandLandmarkerOptions = mp.tasks.vision.HandLandmarkerOptions\n",
    "VisionRunningMode = mp.tasks.vision.RunningMode\n",
    "\n",
    "# Create a hand landmarker instance with the image mode:\n",
    "options = HandLandmarkerOptions(\n",
    "    base_options=BaseOptions(model_asset_path='hand_landmarker.task'),\n",
    "    running_mode=VisionRunningMode.IMAGE)\n",
    "landmarker = HandLandmarker.create_from_options(options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_image(filePath):    \n",
    "    # Load the input image\n",
    "    image = mp.Image.create_from_file(filePath)\n",
    "\n",
    "    # Detect hand landmarks from the input image\n",
    "    detection_result = landmarker.detect(image)\n",
    "\n",
    "    # If no hand detected\n",
    "    if(not detection_result.handedness):\n",
    "        # Convert image to numpy array\n",
    "        annotated_image = np.copy(image.numpy_view())\n",
    "        \n",
    "        # Resize the image\n",
    "        annotated_image = cv2.resize(annotated_image, (480, 480))\n",
    "        \n",
    "        # Define the font settings\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        font_scale = 1.0\n",
    "        font_color = (255, 255, 255)  # White color\n",
    "        thickness = 2\n",
    "\n",
    "        # Add the text to the image\n",
    "        cv2.putText(annotated_image, 'nothing', (10, 50), font, font_scale, font_color, thickness)\n",
    "\n",
    "        # Display the image\n",
    "        cv2.imshow('Prediction', cv2.cvtColor(annotated_image, cv2.COLOR_RGB2BGR))\n",
    "        cv2.waitKey(0)  # Wait until a key is pressed\n",
    "        cv2.destroyAllWindows()  # Close the window\n",
    "        return\n",
    "    \n",
    "    # If hand detected\n",
    "    # Get landmarks and handedness\n",
    "    landmarks = detection_result.hand_world_landmarks[0]\n",
    "    handedness = detection_result.handedness[0][0].category_name\n",
    "    \n",
    "    # Turn landmarks into numpy array\n",
    "    arr = landmark_array(landmarks)\n",
    "\n",
    "    # Reshape the test landmark to match the expected input shape\n",
    "    arr = np.expand_dims(arr, axis=0)\n",
    "\n",
    "    # Predict label\n",
    "    if(handedness == 'Left'):\n",
    "        prediction = model_left.predict(arr)\n",
    "    else:\n",
    "        prediction = model_right.predict(arr)\n",
    "\n",
    "    # Get the predicted label\n",
    "    predicted_class = labels[np.argmax(prediction, axis=1)[0]]\n",
    "\n",
    "    # Draw landmarks and resize image\n",
    "    annotated_image = draw_landmarks_on_image(\n",
    "        image.numpy_view(), detection_result)\n",
    "    annotated_image = cv2.resize(annotated_image, (480, 480))\n",
    "\n",
    "    # Define the font settings\n",
    "    font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "    font_scale = 1.0\n",
    "    font_color = (255, 255, 255)  # White color\n",
    "    thickness = 2\n",
    "\n",
    "    # Add the text to the image\n",
    "    cv2.putText(annotated_image, predicted_class, (10, 50), font, font_scale, font_color, thickness)\n",
    "\n",
    "    # Display the image\n",
    "    cv2.imshow('Prediction', cv2.cvtColor(annotated_image, cv2.COLOR_RGB2BGR))\n",
    "    cv2.waitKey(0)  # Wait until a key is pressed\n",
    "    cv2.destroyAllWindows()  # Close the window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 14ms/step\n"
     ]
    }
   ],
   "source": [
    "predict_image('image.jpg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 15ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 14ms/step\n",
      "1/1 [==============================] - 0s 13ms/step\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "for file in os.listdir('asl_alphabet_test_left'):\n",
    "    predict_image(os.path.join('asl_alphabet_test_left', file))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mediapipe",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
